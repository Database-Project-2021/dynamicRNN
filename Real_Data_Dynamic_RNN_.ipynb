{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic RNN Make Pratical\n",
    "\n",
    "Modify from this [notebook](https://www.kaggle.com/winternguyen/predict-household-electric-power-using-lstms)\n",
    "\n",
    "Download the dataset from this [link](https://www.kaggle.com/uciml/electric-power-consumption-data-set) and put this dataset under the `dataset` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0303,
     "end_time": "2021-05-22T14:20:47.598333",
     "exception": false,
     "start_time": "2021-05-22T14:20:47.568033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "In this Notebook, I try to learn and build the Long Short-Term Memory (LSTM) recurrent neural network to fit one third of data and then predict the rest of data.\n",
    "    \n",
    "Database information:\n",
    "    \n",
    "(1) date: Date in format dd/mm/yyyy\n",
    "\n",
    "(2) time: time in format hh:mm:ss\n",
    "\n",
    "(3) global_active_power: household global minute-averaged active power (in kilowatt)\n",
    "\n",
    "(4) global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
    "\n",
    "(5) voltage: minute-averaged voltage (in volt)\n",
    "\n",
    "(6) global_intensity: household global minute-averaged current intensity (in ampere)\n",
    "\n",
    "(7) sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
    "\n",
    "(8) sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
    "\n",
    "(9) sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.050066,
     "end_time": "2021-05-22T14:20:47.676992",
     "exception": false,
     "start_time": "2021-05-22T14:20:47.626926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/household_power_consumption.txt\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-test.zip\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-train.zip\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-cpu-time-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-dependencies.txt\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-diskio-count-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-features.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-latency-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-networkin-size-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx-testing-data/training-data/transaction-networkout-size-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-cpu-time-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-dependencies.txt\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-diskio-count-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-features.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-latency-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-networkin-size-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-networkout-size-server-0.csv\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-features.pkl\n",
      "dataset/mmt-rte-45-server-1-no-dist-tx/training-data/latency.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import display\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028972,
     "end_time": "2021-05-22T14:20:47.735666",
     "exception": false,
     "start_time": "2021-05-22T14:20:47.706694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import & cleaning data\n",
    "\n",
    "Importing the txt file takes more time than that of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cost_estimator import Loader, Preprocessor, RegressionModels, Task\n",
    "\n",
    "def get_df(dataset_path: Union[str, list]):\n",
    "    ou_name = 'OU2 - Initialize Thread'\n",
    "    task_name = 'latency'\n",
    "    features = ['System CPU Load', 'Process CPU Load',\n",
    "                'System Load Average', 'Thread Active Count']\n",
    "    sample_size = 10000\n",
    "\n",
    "    task = Task(ou_name, task_name)\n",
    "    if isinstance(dataset_path, str):\n",
    "        dataset_path = [dataset_path]\n",
    "        \n",
    "    df_list = []\n",
    "\n",
    "    for path in dataset_path:\n",
    "        # sub_task_name = f'basic1-RTE-{i}'\n",
    "\n",
    "        loader = Loader(path, server_count=1, n_jobs=8)\n",
    "        df_features = loader.load_features_as_df(load_from_pkl=True)\n",
    "        df_latencies = loader.load_latencies_as_df(load_from_pkl=True)\n",
    "        dict_depend = loader.load_dependencies_as_dict()\n",
    "        \n",
    "        df_list.append((df_features, df_latencies, dict_depend))\n",
    "\n",
    "        # print(f'\\n\\n---------------------- Basic1-RTE-{i} Latency ----------------------')\n",
    "        psr = Preprocessor(df_features, df_latencies, ou_name)\n",
    "        psr.drop_warmup_data()\n",
    "\n",
    "        mean_before_filter = float(psr.get_label().mean())\n",
    "        std_before_filter = float(psr.get_label().std())\n",
    "\n",
    "        psr.drop_outlier_and_na(drop_alg='std')\n",
    "\n",
    "        mean_after_filter = float(psr.get_label().mean())\n",
    "        std_after_filter = float(psr.get_label().std())\n",
    "\n",
    "        psr.sample_features(sample_size)\n",
    "        psr.specify_features(features)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load features from dataset/mmt-rte-45-server-1-no-dist-tx/training-data/transaction-features.pkl\n",
      "Load label from dataset/mmt-rte-45-server-1-no-dist-tx/training-data/latency.pkl\n",
      "Drop warmup size: 81808, Size before/after drop warmup data: 265500/183692\n",
      "Drop na size: 0, Size before/after drop na data: 183692/183692\n",
      "Drop outlier size: 23699, Size before/after drop outlier data: 183692/159993\n",
      "Drop sample size: 159993, Size before/after sample: 159993/10000\n",
      "current feature columns: ['System CPU Load', 'Process CPU Load', 'System Load Average', 'Thread Active Count']\n"
     ]
    }
   ],
   "source": [
    "df_list = get_df(dataset_path=['dataset/mmt-rte-45-server-1-no-dist-tx/training-data'])\n",
    "\n",
    "# display(df_features)\n",
    "# display(df_latencies)\n",
    "# print(dict_depend[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_NaN(df):\n",
    "    df.isnull().sum()\n",
    "    df = df.fillna(df.mean())\n",
    "    df.isnull().sum()\n",
    "    # return df\n",
    "\n",
    "def make_dataset(df_features, df_latencies, dict_depend):\n",
    "    df_start_time = df_features[['Transaction ID', 'Start Time']]\n",
    "    df_total_time = df_latencies[['Total']]\n",
    "    df_feature = df_latencies[['Is Distributed', 'Total', 'OU0 - Broadcast', 'OU0 - ROUTE', 'OU1 - Generate Plan', 'OU2 - Initialize Thread',\n",
    "                               'OU3 - Acquire Locks', 'OU4 - Read from Local', 'OU5M - Read from Remote', 'OU6 - Execute Arithmetic Logic', \n",
    "                               'NonOU - Push to Remote', 'OU7 - Write to Local', 'OU8 - Commit']]\n",
    "    \n",
    "    max_depend_num = 0\n",
    "    depends = []\n",
    "    \n",
    "    for k in dict_depend.keys():\n",
    "        depend_num = len(dict_depend[k])\n",
    "        if max_depend_num < depend_num:\n",
    "            max_depend_num = depend_num\n",
    "        \n",
    "    for index, row in df_features.iterrows():\n",
    "        # print(f\"ID: {row['Transaction ID']} | Start: {row['Start Time']} | Total: {row['Total']}\") \n",
    "        if dict_depend is not None:\n",
    "            depend_txns = dict_depend.get(row['Transaction ID'], None)\n",
    "            \n",
    "#             If no dependent Txns, assign a empty list\n",
    "            if depend_txns is None:\n",
    "                depend_txns = []\n",
    "                \n",
    "            depend_txns_num = len(depend_txns)\n",
    "\n",
    "            if depend_txns_num <= max_depend_num:\n",
    "                depends.append(depend_txns + ([0] * (max_depend_num - depend_txns_num)))\n",
    "            else:\n",
    "#                 If exceed the maximum number of dependenet Txns, truncate\n",
    "                depends.append(depend_txns[:max_depend_num])\n",
    "    \n",
    "    df_depends = pd.DataFrame(depends, columns=[f\"dep-{i}\" for i in range(1, max_depend_num + 1)])\n",
    "    df_concat = pd.concat([df_start_time, df_feature, df_depends], axis=1)\n",
    "    df_concat['Is Distributed'] = df_concat['Is Distributed'].astype(int)\n",
    "    \n",
    "    df_X = df_concat\n",
    "    df_Y = pd.DataFrame(df_features['Start Time'] + df_total_time['Total'], columns=['End Time'])\n",
    "    \n",
    "    return df_X, df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_X shape: (265500, 72)\n",
      "df_train_Y shape: (265500, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Is Distributed</th>\n",
       "      <th>Total</th>\n",
       "      <th>OU0 - Broadcast</th>\n",
       "      <th>OU0 - ROUTE</th>\n",
       "      <th>OU1 - Generate Plan</th>\n",
       "      <th>OU2 - Initialize Thread</th>\n",
       "      <th>OU3 - Acquire Locks</th>\n",
       "      <th>OU4 - Read from Local</th>\n",
       "      <th>...</th>\n",
       "      <th>dep-48</th>\n",
       "      <th>dep-49</th>\n",
       "      <th>dep-50</th>\n",
       "      <th>dep-51</th>\n",
       "      <th>dep-52</th>\n",
       "      <th>dep-53</th>\n",
       "      <th>dep-54</th>\n",
       "      <th>dep-55</th>\n",
       "      <th>dep-56</th>\n",
       "      <th>dep-57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>422772</td>\n",
       "      <td>27650</td>\n",
       "      <td>8166</td>\n",
       "      <td>3002</td>\n",
       "      <td>509</td>\n",
       "      <td>180</td>\n",
       "      <td>331502</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>268962</td>\n",
       "      <td>84840</td>\n",
       "      <td>48902</td>\n",
       "      <td>134</td>\n",
       "      <td>328</td>\n",
       "      <td>18</td>\n",
       "      <td>202749</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>378067</td>\n",
       "      <td>85504</td>\n",
       "      <td>43993</td>\n",
       "      <td>356</td>\n",
       "      <td>1247</td>\n",
       "      <td>91</td>\n",
       "      <td>267447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>408310</td>\n",
       "      <td>86214</td>\n",
       "      <td>46013</td>\n",
       "      <td>343</td>\n",
       "      <td>319</td>\n",
       "      <td>83</td>\n",
       "      <td>278215</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>257324</td>\n",
       "      <td>86674</td>\n",
       "      <td>46520</td>\n",
       "      <td>87</td>\n",
       "      <td>1446</td>\n",
       "      <td>17</td>\n",
       "      <td>183418</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265495</th>\n",
       "      <td>265496</td>\n",
       "      <td>179945291</td>\n",
       "      <td>0</td>\n",
       "      <td>6535</td>\n",
       "      <td>84</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>4133</td>\n",
       "      <td>1926</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265496</th>\n",
       "      <td>265497</td>\n",
       "      <td>179945293</td>\n",
       "      <td>0</td>\n",
       "      <td>9207</td>\n",
       "      <td>126</td>\n",
       "      <td>234</td>\n",
       "      <td>32</td>\n",
       "      <td>551</td>\n",
       "      <td>4440</td>\n",
       "      <td>1582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265497</th>\n",
       "      <td>265498</td>\n",
       "      <td>179946896</td>\n",
       "      <td>0</td>\n",
       "      <td>6374</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>5820</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265498</th>\n",
       "      <td>265499</td>\n",
       "      <td>179946967</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>82</td>\n",
       "      <td>111</td>\n",
       "      <td>7</td>\n",
       "      <td>282</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265499</th>\n",
       "      <td>265500</td>\n",
       "      <td>179949938</td>\n",
       "      <td>0</td>\n",
       "      <td>1383</td>\n",
       "      <td>62</td>\n",
       "      <td>817</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265500 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Transaction ID  Start Time  Is Distributed   Total  OU0 - Broadcast  \\\n",
       "0                    1           0               0  422772            27650   \n",
       "1                    2          84               0  268962            84840   \n",
       "2                    3          95               0  378067            85504   \n",
       "3                    4         102               0  408310            86214   \n",
       "4                    5         108               0  257324            86674   \n",
       "...                ...         ...             ...     ...              ...   \n",
       "265495          265496   179945291               0    6535               84   \n",
       "265496          265497   179945293               0    9207              126   \n",
       "265497          265498   179946896               0    6374               68   \n",
       "265498          265499   179946967               0     894               82   \n",
       "265499          265500   179949938               0    1383               62   \n",
       "\n",
       "        OU0 - ROUTE  OU1 - Generate Plan  OU2 - Initialize Thread  \\\n",
       "0              8166                 3002                      509   \n",
       "1             48902                  134                      328   \n",
       "2             43993                  356                     1247   \n",
       "3             46013                  343                      319   \n",
       "4             46520                   87                     1446   \n",
       "...             ...                  ...                      ...   \n",
       "265495           71                    9                     4133   \n",
       "265496          234                   32                      551   \n",
       "265497           63                   16                       11   \n",
       "265498          111                    7                      282   \n",
       "265499          817                   16                       96   \n",
       "\n",
       "        OU3 - Acquire Locks  OU4 - Read from Local  ...  dep-48  dep-49  \\\n",
       "0                       180                 331502  ...       0       0   \n",
       "1                        18                 202749  ...       0       0   \n",
       "2                        91                 267447  ...       0       0   \n",
       "3                        83                 278215  ...       0       0   \n",
       "4                        17                 183418  ...       0       0   \n",
       "...                     ...                    ...  ...     ...     ...   \n",
       "265495                 1926                    107  ...       0       0   \n",
       "265496                 4440                   1582  ...       0       0   \n",
       "265497                 5820                    117  ...       0       0   \n",
       "265498                    2                    135  ...       0       0   \n",
       "265499                    2                    114  ...       0       0   \n",
       "\n",
       "        dep-50  dep-51  dep-52  dep-53  dep-54  dep-55  dep-56  dep-57  \n",
       "0            0       0       0       0       0       0       0       0  \n",
       "1            0       0       0       0       0       0       0       0  \n",
       "2            0       0       0       0       0       0       0       0  \n",
       "3            0       0       0       0       0       0       0       0  \n",
       "4            0       0       0       0       0       0       0       0  \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "265495       0       0       0       0       0       0       0       0  \n",
       "265496       0       0       0       0       0       0       0       0  \n",
       "265497       0       0       0       0       0       0       0       0  \n",
       "265498       0       0       0       0       0       0       0       0  \n",
       "265499       0       0       0       0       0       0       0       0  \n",
       "\n",
       "[265500 rows x 72 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>378162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>257432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265495</th>\n",
       "      <td>179951826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265496</th>\n",
       "      <td>179954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265497</th>\n",
       "      <td>179953270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265498</th>\n",
       "      <td>179947861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265499</th>\n",
       "      <td>179951321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         End Time\n",
       "0          422772\n",
       "1          269046\n",
       "2          378162\n",
       "3          408412\n",
       "4          257432\n",
       "...           ...\n",
       "265495  179951826\n",
       "265496  179954500\n",
       "265497  179953270\n",
       "265498  179947861\n",
       "265499  179951321\n",
       "\n",
       "[265500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_X, df_train_Y = make_dataset(df_features=df_list[0][0], df_latencies=df_list[0][1], dict_depend=df_list[0][2])\n",
    "\n",
    "print(f\"df_train_X shape: {df_train_X.shape}\")\n",
    "print(f\"df_train_Y shape: {df_train_Y.shape}\")\n",
    "display(df_train_X)\n",
    "display(df_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046777,
     "end_time": "2021-05-22T14:21:11.051238",
     "exception": false,
     "start_time": "2021-05-22T14:21:11.004461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# values = df_resample.values\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled = scaler.fit_transform(values)\n",
    "# reframed = series_to_supervised(scaled, 1, 1)\n",
    "# r = list(range(df_resample.shape[1]+1, 2*df_resample.shape[1]))\n",
    "# reframed.drop(reframed.columns[r], axis=1, inplace=True)\n",
    "# reframed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(seq_len: int, dataset: pd.DataFrame, seq_num: int=None):\n",
    "    # Set the Length of Sequence\n",
    "    seq_len = 200\n",
    "\n",
    "    # Data spliting into train and test data series. Only 4000 first data points are selected for traing purpose.\n",
    "    values = dataset.values\n",
    "    n = values.shape[0]\n",
    "    if seq_num is None:\n",
    "        seq_num = n // seq_len\n",
    "    series_n = seq_num * seq_len\n",
    "    print(f\"Dataset shape: {values.shape}, Length of Sequence: {seq_len}, Number of Sequence: {seq_num}, Number of Series: {series_n}\")\n",
    "    values_truncated = values[:series_n]\n",
    "\n",
    "    # n_train_time = 4000\n",
    "    # train = values_truncated[:n_train_time, :]\n",
    "    # test = values_truncated[n_train_time:, :]\n",
    "    # train_x, train_y = train[:, :-1], train[:, -1]\n",
    "    # test_x, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "    # Shape: [number of sequences, sequence lengh, feature size]\n",
    "    # train_x = train_x.reshape((train_x.shape[0] // seq_len, seq_len, train_x.shape[1]))\n",
    "    # test_x = test_x.reshape((test_x.shape[0] // seq_len, seq_len, test_x.shape[1]))\n",
    "    # train_y = train_y.reshape((train_y.shape[0] // seq_len, seq_len))\n",
    "    # test_y = test_y.reshape((test_y.shape[0] // seq_len, seq_len))\n",
    "    \n",
    "    df_dataset = values_truncated.reshape((values_truncated.shape[0] // seq_len, seq_len, values_truncated.shape[1]))\n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "papermill": {
     "duration": 0.155047,
     "end_time": "2021-05-22T14:21:11.949520",
     "exception": false,
     "start_time": "2021-05-22T14:21:11.794473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (265500, 72), Length of Sequence: 200, Number of Sequence: 1327, Number of Series: 265400\n",
      "Dataset shape: (265500, 1), Length of Sequence: 200, Number of Sequence: 1327, Number of Series: 265400\n",
      "Dataset shape: (265500, 72), Length of Sequence: 200, Number of Sequence: 1327, Number of Series: 265400\n",
      "Dataset shape: (265500, 1), Length of Sequence: 200, Number of Sequence: 1327, Number of Series: 265400\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "\n",
    "train_x, train_y = reshape_dataset(seq_len=seq_len, dataset=df_train_X), reshape_dataset(seq_len=seq_len, dataset=df_train_Y)\n",
    "test_x, test_y = reshape_dataset(seq_len=seq_len, dataset=df_train_X), reshape_dataset(seq_len=seq_len, dataset=df_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (1327, 200, 72)\n",
      "train_y shape: (1327, 200, 1)\n",
      "test_x shape: (1327, 200, 72)\n",
      "test_y shape: (1327, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'train_x shape: {train_x.shape}')\n",
    "print(f'train_y shape: {train_y.shape}')\n",
    "print(f'test_x shape: {test_x.shape}')\n",
    "print(f'test_y shape: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047771,
     "end_time": "2021-05-22T14:21:12.045822",
     "exception": false,
     "start_time": "2021-05-22T14:21:11.998051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3> LSTM model setting <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047984,
     "end_time": "2021-05-22T14:21:12.142150",
     "exception": false,
     "start_time": "2021-05-22T14:21:12.094166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(1) 100 neurons in the first visible layer\n",
    "\n",
    "(2) dropout 10%\n",
    "\n",
    "(3) 1 neuron in the output layer for predicting Global_active_power\n",
    "\n",
    "(4) The input shape will be 1 time step with 7 features\n",
    "\n",
    "(5) The mean_squared_error loss function and the efficient adam version of stochastic gradient descent\n",
    "\n",
    "(6) The model will be fit for 50 training epochs with a batch size of 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense, RNN, Input\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        super(VanillaRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.tensordot(inputs, self.kernel, axes=1)\n",
    "        output = h + tf.tensordot(prev_output, self.recurrent_kernel, axes=1)\n",
    "        return output, [output]\n",
    "    \n",
    "def getVanillaRNN(inputShape):\n",
    "    rnnLayer = RNN(VanillaRNNCell(100), dynamic=True, return_sequences=True, return_state=False)\n",
    "\n",
    "    inputs = Input(shape=inputShape)\n",
    "    x = rnnLayer(inputs)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Unit Dynamic RNN\n",
    "Howeve, since the hidden state contains only 1 unit, the capacity is limited. The performance is quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIngle unit output RNN\n",
    "class SingleUnitDynamicRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, rnn_units=100, memory_size=3, **kwargs):\n",
    "        super(SingleUnitDynamicRNNCell, self).__init__(**kwargs)\n",
    "        # Must have variables\n",
    "        self.units = units\n",
    "        self.state_size = tf.TensorShape([memory_size, units])\n",
    "\n",
    "        # Custom variables\n",
    "        self.rnn_units = rnn_units\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.rnn_units),\n",
    "            initializer='uniform',\n",
    "            name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.rnn_units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.scalar_dense = self.add_weight(\n",
    "            shape=(self.rnn_units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='scalar_dense')\n",
    "        self.built = True\n",
    "    \n",
    "    def generate_indice(self, states):\n",
    "        batch_size = states.shape[0]\n",
    "        # Random select\n",
    "        return tf.random.uniform((batch_size, 1), minval=0, maxval=self.memory_size, dtype=tf.dtypes.int32)\n",
    "\n",
    "    def select_memory(self, states):\n",
    "        batch_size = states.shape[0]\n",
    "        indice = self.generate_indice(states)\n",
    "\n",
    "        # The sequence number for Indices\n",
    "        sequence = tf.range(start=0, limit=batch_size)\n",
    "        sequence = tf.reshape(sequence, (sequence.shape[0], 1))\n",
    "\n",
    "        # Indices for gather_nd\n",
    "        gather_indice = tf.concat([sequence, indice], axis=1)\n",
    "\n",
    "        # Grab the selected datas according to the indice\n",
    "        selected_state = tf.gather_nd(states, gather_indice)\n",
    "        return selected_state\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "\n",
    "        # Recurrent NN\n",
    "        h = tf.tensordot(inputs, self.kernel, axes=1)\n",
    "        selected_state = self.select_memory(states[0])\n",
    "        x = relu(h + tf.tensordot(selected_state, self.recurrent_kernel, axes=1))\n",
    "        output = tf.tensordot(x, self.scalar_dense, axes=1)\n",
    "\n",
    "        # Concat new state\n",
    "        new_state = tf.reshape(output, (output.shape[0], 1, output.shape[1]))\n",
    "        output_states = tf.concat([new_state, prev_output], axis=1)\n",
    "        output_states = output_states[:, 0:self.memory_size, :]\n",
    "\n",
    "        return output, [output_states]\n",
    "\n",
    "def getSingleUnitRNN(inputShape, memory_size):\n",
    "    rnnLayer = RNN(SingleUnitDynamicRNNCell(1, rnn_units=100, memory_size=memory_size), dynamic=True, return_sequences=True, return_state=False)\n",
    "\n",
    "    inputs = Input(shape=inputShape)\n",
    "    outputs = rnnLayer(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Units Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, memory_size=3, **kwargs):\n",
    "        super(DynamicRNNCell, self).__init__(**kwargs)\n",
    "        # Must have variables\n",
    "        self.units = units\n",
    "        self.state_size = tf.TensorShape([memory_size, units])\n",
    "\n",
    "        # Custom variables\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "    \n",
    "    def generate_indice(self, states):\n",
    "        batch_size = states.shape[0]\n",
    "        # Random select\n",
    "        return tf.random.uniform((batch_size, 1), minval=0, maxval=self.memory_size, dtype=tf.dtypes.int32)\n",
    "\n",
    "    def select_memory(self, states):\n",
    "        batch_size = states.shape[0]\n",
    "        indice = self.generate_indice(states)\n",
    "\n",
    "        # The sequence number for Indices\n",
    "        sequence = tf.range(start=0, limit=batch_size)\n",
    "        sequence = tf.reshape(sequence, (sequence.shape[0], 1))\n",
    "\n",
    "        # Indices for gather_nd\n",
    "        gather_indice = tf.concat([sequence, indice], axis=1)\n",
    "\n",
    "        # Grab the selected datas according to the indice\n",
    "        selected_state = tf.gather_nd(states, gather_indice)\n",
    "        return selected_state\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "\n",
    "        # Recurrent NN\n",
    "        h = tf.tensordot(inputs, self.kernel, axes=1)\n",
    "        selected_state = self.select_memory(states[0])\n",
    "        output = h + tf.tensordot(selected_state, self.recurrent_kernel, axes=1)\n",
    "\n",
    "        # Concat new state\n",
    "        new_state = tf.reshape(output, (output.shape[0], 1, output.shape[1]))\n",
    "        output_states = tf.concat([new_state, prev_output], axis=1)\n",
    "        output_states = output_states[:, 0:self.memory_size, :]\n",
    "\n",
    "        return output, [output_states]\n",
    "\n",
    "def getModel(inputShape, memory_size):\n",
    "    rnnLayer = RNN(DynamicRNNCell(100, memory_size=memory_size), dynamic=True, return_sequences=True, return_state=False)\n",
    "\n",
    "    inputs = Input(shape=inputShape)\n",
    "    x = rnnLayer(inputs)\n",
    "    x = relu(x)\n",
    "    x = Dense(100)(x)\n",
    "    x = relu(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 40.116431,
     "end_time": "2021-05-22T14:21:52.306797",
     "exception": false,
     "start_time": "2021-05-22T14:21:12.190366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 200, 72)]         0         \n",
      "_________________________________________________________________\n",
      "rnn_4 (RNN)                  (None, 200, 3, 100)       0 (unused)\n",
      "_________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)    (None, 200, 3, 100)       0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200, 3, 100)       10100     \n",
      "_________________________________________________________________\n",
      "tf.nn.relu_3 (TFOpLambda)    (None, 200, 3, 100)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200, 3, 100)       0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200, 3, 1)         101       \n",
      "=================================================================\n",
      "Total params: 10,201\n",
      "Trainable params: 10,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer DynamicRNNCell has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 00:20:44.855520: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-07 00:20:44.855552: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-07 00:20:44.855674: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-12-07 00:20:44.855972: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-07 00:20:44.855986: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-12-07 00:20:45.630358: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-07 00:20:45.630401: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-07 00:20:45.630437: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-12-07 00:20:46.070295: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-07 00:20:46.071177: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-12-07 00:20:46.139074: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-12-07 00:20:46.203474: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-07 00:20:46.361227: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46\n",
      "\n",
      "2021-12-07 00:20:46.477134: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.trace.json.gz\n",
      "2021-12-07 00:20:46.585338: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46\n",
      "\n",
      "2021-12-07 00:20:46.590147: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.memory_profile.json.gz\n",
      "2021-12-07 00:20:46.591301: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46\n",
      "Dumped tool data for xplane.pb to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/20211207-002044/train/plugins/profile/2021_12_07_00_20_46/netdb-bujoplus0.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 13s - loss: 5423060857389056.0000 - val_loss: 1732956490039296.0000\n",
      "Epoch 2/50\n",
      "19/19 - 12s - loss: 1024539148419072.0000 - val_loss: 292798758649856.0000\n",
      "Epoch 3/50\n",
      "19/19 - 12s - loss: 196827345321984.0000 - val_loss: 107665854300160.0000\n",
      "Epoch 4/50\n",
      "19/19 - 12s - loss: 141273436520448.0000 - val_loss: 76101133008896.0000\n",
      "Epoch 5/50\n",
      "19/19 - 12s - loss: 125645040386048.0000 - val_loss: 68192261111808.0000\n",
      "Epoch 6/50\n",
      "19/19 - 12s - loss: 113610089037824.0000 - val_loss: 58246081544192.0000\n",
      "Epoch 7/50\n",
      "19/19 - 12s - loss: 100778513530880.0000 - val_loss: 48817655250944.0000\n",
      "Epoch 8/50\n",
      "19/19 - 12s - loss: 90346071523328.0000 - val_loss: 39569172987904.0000\n",
      "Epoch 9/50\n",
      "19/19 - 12s - loss: 80023411228672.0000 - val_loss: 30961752342528.0000\n",
      "Epoch 10/50\n",
      "19/19 - 12s - loss: 71493094473728.0000 - val_loss: 23531368742912.0000\n",
      "Epoch 11/50\n",
      "19/19 - 12s - loss: 63402797957120.0000 - val_loss: 16872510586880.0000\n",
      "Epoch 12/50\n",
      "19/19 - 12s - loss: 56713520611328.0000 - val_loss: 12287447400448.0000\n",
      "Epoch 13/50\n",
      "19/19 - 12s - loss: 52107168186368.0000 - val_loss: 8429515571200.0000\n",
      "Epoch 14/50\n",
      "19/19 - 12s - loss: 48536708186112.0000 - val_loss: 6078014685184.0000\n",
      "Epoch 15/50\n",
      "19/19 - 12s - loss: 45800834990080.0000 - val_loss: 4204466274304.0000\n",
      "Epoch 16/50\n",
      "19/19 - 12s - loss: 44011477794816.0000 - val_loss: 3154982469632.0000\n",
      "Epoch 17/50\n",
      "19/19 - 12s - loss: 42190457798656.0000 - val_loss: 2184740601856.0000\n",
      "Epoch 18/50\n",
      "19/19 - 12s - loss: 40761047384064.0000 - val_loss: 1519955476480.0000\n",
      "Epoch 19/50\n",
      "19/19 - 12s - loss: 39539171131392.0000 - val_loss: 1172363280384.0000\n",
      "Epoch 20/50\n",
      "19/19 - 12s - loss: 38743771709440.0000 - val_loss: 684374228992.0000\n",
      "Epoch 21/50\n",
      "19/19 - 12s - loss: 37581135806464.0000 - val_loss: 356742594560.0000\n",
      "Epoch 22/50\n",
      "19/19 - 12s - loss: 36934407684096.0000 - val_loss: 308079919104.0000\n",
      "Epoch 23/50\n",
      "19/19 - 12s - loss: 36162597027840.0000 - val_loss: 251967946752.0000\n",
      "Epoch 24/50\n",
      "19/19 - 12s - loss: 35498022141952.0000 - val_loss: 189948608512.0000\n",
      "Epoch 25/50\n",
      "19/19 - 12s - loss: 35320787632128.0000 - val_loss: 82815000576.0000\n",
      "Epoch 26/50\n",
      "19/19 - 12s - loss: 34937273057280.0000 - val_loss: 200609021952.0000\n",
      "Epoch 27/50\n",
      "19/19 - 12s - loss: 34387043287040.0000 - val_loss: 50891710464.0000\n",
      "Epoch 28/50\n",
      "19/19 - 12s - loss: 34226338529280.0000 - val_loss: 136795365376.0000\n",
      "Epoch 29/50\n",
      "19/19 - 12s - loss: 33963691212800.0000 - val_loss: 266413309952.0000\n",
      "Epoch 30/50\n",
      "19/19 - 12s - loss: 33968283975680.0000 - val_loss: 179373654016.0000\n",
      "Epoch 31/50\n",
      "19/19 - 12s - loss: 33670595346432.0000 - val_loss: 145918509056.0000\n",
      "Epoch 32/50\n",
      "19/19 - 12s - loss: 33354288201728.0000 - val_loss: 30582114304.0000\n",
      "Epoch 33/50\n",
      "19/19 - 12s - loss: 33238019997696.0000 - val_loss: 60948123648.0000\n",
      "Epoch 34/50\n",
      "19/19 - 12s - loss: 32811752882176.0000 - val_loss: 304471867392.0000\n",
      "Epoch 35/50\n",
      "19/19 - 12s - loss: 32625383178240.0000 - val_loss: 158162796544.0000\n",
      "Epoch 36/50\n",
      "19/19 - 12s - loss: 32377101352960.0000 - val_loss: 141483163648.0000\n",
      "Epoch 37/50\n",
      "19/19 - 12s - loss: 32382008688640.0000 - val_loss: 34126469120.0000\n",
      "Epoch 38/50\n",
      "19/19 - 13s - loss: 32253602168832.0000 - val_loss: 48656343040.0000\n",
      "Epoch 39/50\n",
      "19/19 - 12s - loss: 32183469211648.0000 - val_loss: 149839446016.0000\n",
      "Epoch 40/50\n",
      "19/19 - 12s - loss: 32213160689664.0000 - val_loss: 137315467264.0000\n",
      "Epoch 41/50\n",
      "19/19 - 12s - loss: 32140901220352.0000 - val_loss: 157401923584.0000\n",
      "Epoch 42/50\n",
      "19/19 - 12s - loss: 32158689263616.0000 - val_loss: 52649582592.0000\n",
      "Epoch 43/50\n",
      "19/19 - 12s - loss: 32049022894080.0000 - val_loss: 138860052480.0000\n",
      "Epoch 44/50\n",
      "19/19 - 12s - loss: 32136874688512.0000 - val_loss: 176540844032.0000\n",
      "Epoch 45/50\n",
      "19/19 - 12s - loss: 32088606638080.0000 - val_loss: 104680767488.0000\n",
      "Epoch 46/50\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "batch_size = 70\n",
    "memory_size = 3\n",
    "\n",
    "# model = getVanillaRNN((train_x.shape[1], train_x.shape[2]))\n",
    "model = getModel((train_x.shape[1], train_x.shape[2]), memory_size=memory_size)\n",
    "# model = getSingleUnitRNN((train_x.shape[1], train_x.shape[2]), memory_size=memory_size)\n",
    "\n",
    "# Network fitting\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "history = model.fit(train_x, train_y, \n",
    "                    epochs=50, batch_size=batch_size, \n",
    "                    validation_data=(test_x, test_y), verbose=2, \n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Loss history plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we have resampled the database into hour, so, every time step is one hour. We try first to check the prediction in 500 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = test_x.shape[2]\n",
    "\n",
    "# Prediction test\n",
    "yhat = model.predict(test_x)\n",
    "yhat = yhat.reshape((-1, 1))\n",
    "print(yhat.shape)\n",
    "test_x_reshape = test_x.reshape((-1, size))\n",
    "print(test_x_reshape.shape)\n",
    "\n",
    "# invert scaling for prediction\n",
    "inv_yhat = np.concatenate((yhat, test_x_reshape[:, 1-size:]), axis=1)\n",
    "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y_reshape = test_y.reshape(-1, 1)\n",
    "inv_y = np.concatenate((test_y_reshape, test_x_reshape[:, 1-size:]), axis=1)\n",
    "# inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "aa=[x for x in range(500)]\n",
    "plt.figure(figsize=(25,10)) \n",
    "plt.plot(aa, inv_y[:500], marker='.', label=\"actual\")\n",
    "plt.plot(aa, inv_yhat[:500], 'r', label=\"prediction\")\n",
    "plt.ylabel(df_train_Y.columns[0], size=15)\n",
    "plt.xlabel('Time step for first 500 hours', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Tensorflow 2.0 APIs\n",
    "- [tf.range](https://www.tensorflow.org/api_docs/python/tf/range)\n",
    "- [tf.gather_nd](https://www.tensorflow.org/api_docs/python/tf/gather_nd)\n",
    "- [tf.keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)\n",
    "- [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "  - Also contain the example code of `return_sequences` and `return_state`\n",
    "- [tf.tensordot](https://www.tensorflow.org/api_docs/python/tf/tensordot)\n",
    "- [Introduction to tensor slicing](https://www.tensorflow.org/guide/tensor_slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "987a5ef3c75d83e8e570e4446fbb6c608ccd6899a067e7316a9d4cbabc373ada"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "duration": 355.466759,
   "end_time": "2021-05-22T14:26:38.471893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-22T14:20:43.005134",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
